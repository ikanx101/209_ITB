---
title: "Untitled"
author: "Mohammad Rizka Fadhli"
date: "5/19/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())

library(dplyr)
library(tidyr)
```

\newpage

# SOAL DAN PEMBAHASAN

## Soal I

Diberikan 10 buah titik data sebagai berikut:

```{r,echo=FALSE}
df = data.frame(
  titik = paste0("p",1:10),
  x = c(4,2.1,3.4,2.7,.8,4.6,4.3,2.2,4.1,1.5),
  y = c(5.2,3.9,3.1,2,4.1,2.9,1.2,1,4.1,3)
)

df %>% knitr::kable(caption = "Data Soal I")
```

- Lakukan klasterisasi dari data tersebut dengan menggunakan algoritma _k-means_ dengan jumlah partisi $K=2$ sebanyak 10 kali.
- Tentukan sentroid awal (secara _random_) yang berbeda setiap melakukan klasterisasi.
- _Stopping criteria_ untuk klasterisasi bisa ditentukan sendiri (tidak harus sampai tidak ada perubahan sentroid)

### Pertanyaan

1. Tuliskan hasil akhir kluster yang didapat untuk setiap klasterisasi!
1. Hitung nilai _average_ ___SSE___ untuk masing-masing hasil klusterisasi!
1. Hitung nilai _average_ ___Sillhouette Coefficient___ untuk masing-masing hasil klusterisasi!
1. Dari hasil ___SSE___ dan ___Sillhouette Coefficient___, menurut Anda, hasil klasterisasi mana
yang memberikan hasil terbaik? Berikan alasannya!
1. Apakah algoritma _K-means_ sudah memberikan hasil yang baik? Apa yang dapat dilakukan agar hasil klasterisasi lebih baik?

### Pembahasan

Untuk melakukan _k-means clustering_ ini, saya akan membuat algoritma sendiri dengan menggunakan 2 titik _random_ dan akan dilakukan sebanyak 10 kali.

```{r}
# program untuk membuat titik sentroid secara random
random_titik = function(){
  list(
    sentroid_1 = runif(2,0,6),
    sentroid_2 = runif(2,0,6)
  )
}

# program untuk menghitung jarak
jarak = function(x1,x2){
  sb_1 = (x1[1] - x2[1])^2
  sb_2 = (x1[2] - x2[2])^2
  sqrt(sb_1 + sb_2)
}
```


```{r}
# iterasi pertama
random = random_titik()
sentroid_1 = random$sentroid_1
sentroid_2 = random$sentroid_2

df$jarak_sentroid1 = NA
df$jarak_sentroid2 = NA

for(i in 1:nrow(df)){
  titik = c(df$x[i],df$y[i])
  df$jarak_sentroid1[i] = jarak(titik,sentroid_1)
  df$jarak_sentroid2[i] = jarak(titik,sentroid_2)
}

df = 
  df %>% 
  mutate(membership = ifelse(jarak_sentroid1 < jarak_sentroid2,1,2))

```

\newpage

## Soal II

Diberikan _confusion matrix_ sebagai berikut:

```{r,echo=FALSE}
df = data.frame(
  cluster = c("#1","#2","#3","Total"),
  entertainment = c(1,27,326,354),
  financial = c(1,89,465,555),
  foreign = c(0,333,8,341),
  metro = c(11,827,105,943),
  national = c(4,253,16,273),
  sports = c(676,33,29,738),
  Total = c(693,1562,949,3204)
)

df %>% knitr::kable(align = "c",caption = "Data Soal II")
```

### Pertanyaan

Hitung nilai _entropy_ dan _purity_ untuk matriks tersebut! Berikan analisis untuk hasil yang didapat!

### Pembahasan

Entropi untuk masing-masing cluster dihitung sebagai berikut:

$$
\begin{align*}
\text{Entropy 1} = & - \frac{1}{693} \log_2 (\frac{1}{693}) - \frac{1}{693} \log_2 (\frac{1}{693}) \\
                   & - 0 - \frac{11}{693} \log_2 (\frac{11}{693}) \\
                   & - \frac{4}{693} \log_2 (\frac{4}{693}) - \frac{676}{693} \log_2 (\frac{676}{693}) \\
                 = & 0.200
\end{align*}
$$


$$
\begin{align*}
\text{Entropy 2} = & - \frac{27}{1562} \log_2 (\frac{27}{1562}) - \frac{89}{1562} \log_2 (\frac{89}{1562}) \\
                   & - \frac{333}{1562} \log_2 (\frac{333}{1562}) - \frac{872}{1562} \log_2 (\frac{872}{1562}) \\
                   & - \frac{253}{1562} \log_2 (\frac{253}{1562}) - \frac{33}{1562} \log_2 (\frac{33}{1562}) \\
                 = & 1.841
\end{align*}

$$


$$
\begin{align*}
\text{Entropy 3} = & - \frac{326}{949} \log_2 (\frac{326}{949}) - \frac{465}{949} \log_2 (\frac{465}{949}) \\
                   & - \frac{8}{949} \log_2 (\frac{8}{949}) - \frac{105}{949} \log_2 (\frac{105}{949}) \\
                   & - \frac{16}{949} \log_2 (\frac{16}{949}) - \frac{29}{949} \log_2 (\frac{29}{949}) \\
                 = & 1.696
\end{align*}

$$


Sedangkan untuk _purity_ dihitung dengan cara:

$$
\begin{align*}
\text{Purity 1} & = \frac{676}{693} &= 0.975 \\
\text{Purity 2} & = \frac{827}{1562} &= 0.529 \\
\text{Purity 3} & = \frac{465}{949} &= 0.490 \\
\end{align*}
$$

_Total entropy_ dihitung sebagai berikut:

$$
\text{Total entropy} = \frac{693 \times 0.200 + 1562 \times 1.841 + 949 \times 0.490}{3204} = 0.614
$$

_Total purity_ dihitung sebagai berikut:

$$
\text{Total purity} = \frac{693 \times 0.975 + 1562 \times 0.529 + 949 \times 1.696}{3204} = 1.443
$$


