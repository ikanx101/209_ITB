---
title: "Untitled"
author: "Mohammad Rizka Fadhli"
date: "5/19/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())

library(dplyr)
library(tidyr)
```

\newpage

# SOAL DAN PEMBAHASAN

## Soal I

Diberikan 10 buah titik data sebagai berikut:

```{r,echo=FALSE}
df = data.frame(
  titik = paste0("p",1:10),
  x = c(4,2.1,3.4,2.7,.8,4.6,4.3,2.2,4.1,1.5),
  y = c(5.2,3.9,3.1,2,4.1,2.9,1.2,1,4.1,3)
)
soal = df
soal %>% knitr::kable(caption = "Data Soal I")
```

- Lakukan klasterisasi dari data tersebut dengan menggunakan algoritma _k-means_ dengan jumlah partisi $K=2$ sebanyak 10 kali.
- Tentukan sentroid awal (secara _random_) yang berbeda setiap melakukan klasterisasi.
- _Stopping criteria_ untuk klasterisasi bisa ditentukan sendiri (tidak harus sampai tidak ada perubahan sentroid)

### Pertanyaan

1. Tuliskan hasil akhir kluster yang didapat untuk setiap klasterisasi!
1. Hitung nilai _average_ ___SSE___ untuk masing-masing hasil klusterisasi!
1. Hitung nilai _average_ ___Sillhouette Coefficient___ untuk masing-masing hasil klusterisasi!
1. Dari hasil ___SSE___ dan ___Sillhouette Coefficient___, menurut Anda, hasil klasterisasi mana
yang memberikan hasil terbaik? Berikan alasannya!
1. Apakah algoritma _K-means_ sudah memberikan hasil yang baik? Apa yang dapat dilakukan agar hasil klasterisasi lebih baik?

### Pembahasan

Untuk melakukan _k-means clustering_ ini, saya akan membuat algoritma sendiri dengan menggunakan 2 titik _random_ dan akan dilakukan sebanyak 10 kali.

```{r}
# program untuk membuat titik sentroid secara random
random_titik = function(){
  list(
    sentroid_1 = runif(2,0,6),
    sentroid_2 = runif(2,0,6)
  )
}

# program untuk menghitung jarak
jarak = function(x1,x2){
  sb_1 = (x1[1] - x2[1])^2
  sb_2 = (x1[2] - x2[2])^2
  sqrt(sb_1 + sb_2)
}

# program untuk menghitung sentroid baru
new_sentroid = function(data){
  hit = 
    data %>% 
    group_by(cluster_no) %>% 
    summarise(x = mean(x),
              y = mean(y)) %>% 
    ungroup()
  output = list(sentroid_1 = c(hit$x[1],hit$y[1]),
                sentroid_2 = c(hit$x[2],hit$y[2]))
  return(output)
}
```


```{r}
# iterasi pertama
random = random_titik()
sentroid_1 = random$sentroid_1
sentroid_2 = random$sentroid_2

df$jarak_sentroid1 = NA
df$jarak_sentroid2 = NA

konvergensi = 1000

while(konvergensi <= 0.005){
  # hitung jarak terhadap sentroid
  for(i in 1:nrow(df)){
    titik = c(df$x[i],df$y[i])
    df$jarak_sentroid1[i] = jarak(titik,sentroid_1)
    df$jarak_sentroid2[i] = jarak(titik,sentroid_2)
  }
  # memasukkan masing-masing titik ke cluster terdekat
  df = 
    df %>% 
    mutate(cluster_no = ifelse(jarak_sentroid1 < jarak_sentroid2,1,2))
  # menghitung sentroid baru
  sentroid_baru = new_sentroid(df)
  # assign ke dalam nama sentroid
  sentroid_1 = sentroid_baru$sentroid_1
  sentroid_2 = sentroid_baru$sentroid_2
}
  
df

```

\newpage

## Soal II

Diberikan _confusion matrix_ sebagai berikut:

```{r,echo=FALSE}
df = data.frame(
  cluster = c("#1","#2","#3","Total"),
  entertainment = c(1,27,326,354),
  financial = c(1,89,465,555),
  foreign = c(0,333,8,341),
  metro = c(11,827,105,943),
  national = c(4,253,16,273),
  sports = c(676,33,29,738),
  Total = c(693,1562,949,3204)
)

df %>% knitr::kable(align = "c",caption = "Data Soal II")
```

### Pertanyaan

Hitung nilai _entropy_ dan _purity_ untuk matriks tersebut! Berikan analisis untuk hasil yang didapat!

### Pembahasan

Entropi untuk masing-masing cluster dihitung sebagai berikut:

$$\begin{matrix}
\text{Entropy 1} = & - \frac{1}{693} \log_2 (\frac{1}{693}) - \frac{1}{693} \log_2 (\frac{1}{693}) \\
                   & - 0 - \frac{11}{693} \log_2 (\frac{11}{693}) \\
                   & - \frac{4}{693} \log_2 (\frac{4}{693}) - \frac{676}{693} \log_2 (\frac{676}{693}) \\
                   & = 0.200
\end{matrix}$$


$$\begin{matrix}
\text{Entropy 2} = & - \frac{27}{1562} \log_2 (\frac{27}{1562}) - \frac{89}{1562} \log_2 (\frac{89}{1562}) \\
                   & - \frac{333}{1562} \log_2 (\frac{333}{1562}) - \frac{872}{1562} \log_2 (\frac{872}{1562}) \\
                   & - \frac{253}{1562} \log_2 (\frac{253}{1562}) - \frac{33}{1562} \log_2 (\frac{33}{1562}) \\
                   & = 1.841
\end{matrix}$$


$$\begin{matrix}
\text{Entropy 3} = & - \frac{326}{949} \log_2 (\frac{326}{949}) - \frac{465}{949} \log_2 (\frac{465}{949}) \\
                   & - \frac{8}{949} \log_2 (\frac{8}{949}) - \frac{105}{949} \log_2 (\frac{105}{949}) \\
                   & - \frac{16}{949} \log_2 (\frac{16}{949}) - \frac{29}{949} \log_2 (\frac{29}{949}) \\
                   & = 1.696
\end{matrix}$$


Sedangkan untuk _purity_ dihitung dengan cara:

$$\begin{matrix}
\text{Purity 1} =& \frac{676}{693} &= 0.975 \\
\text{Purity 2} =& \frac{827}{1562} &= 0.529 \\
\text{Purity 3} =& \frac{465}{949} &= 0.490 \\
\end{matrix}$$

_Total entropy_ dihitung sebagai berikut:

$$\text{Total entropy} = \frac{693 \times 0.200 + 1562 \times 1.841 + 949 \times 0.490}{3204} = 0.614$$

_Total purity_ dihitung sebagai berikut:

$$\text{Total purity} = \frac{693 \times 0.975 + 1562 \times 0.529 + 949 \times 1.696}{3204} = 1.443$$

\newpage

Berikut jika disajikan dalam bentuk tabel:


```{r,echo=FALSE}
df$Entropy = c(0.200,1.841,1.696,0.614)
df$Purity = c(0.975,0.529,0.490,1.443)

df %>% knitr::kable(align = "c",caption = "Hasil Perhitungan Entropy dan Purity")
```

Dari tabel di atas, kita bisa dapatkan informasi sebagai berikut:

> _Cluster_ `#1` memiliki _purity_ yang sangat tinggi dan _entropy_ terendah. Artinya, cluster ini berhasil mengelompokkan data yang _unique_ karakteristiknya (berasal dari satu atribut dominan). Berbeda dengan _cluster_ `#2` dan `#3` yang tidak memiliki satu atribut yang dominan. Tapi secara keseluruhan, _cluster_ yang dihasilkan sudah bisa memisahkan data menjadi 3 kelompok dengan karakteristik yang berbeda-beda.